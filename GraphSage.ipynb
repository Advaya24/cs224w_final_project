{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS224w GraphSage notebook and code\n",
        "The code will download the obgn-mag dataset for use.\n",
        "\n",
        "The code uses a custom version of DGL that is available [here](https://github.com/ali6947/dgl)\n",
        "\n",
        "Or one can modify \"/lib/python3.10/site-packages/dgl/subgraph.py\" on their end to be the subgraph.py in the above repo. The filepath to subgraph.py of the DGL installation would be different in your setup if you use Windows or a virtual environment.\n",
        "\n",
        "Parts of this notebook's code have been adapted from this [tutorial](https://docs.dgl.ai/en/0.8.x/tutorials/blitz/4_link_predict.html)\n",
        "\n",
        "The notebook downloads the obgn-mag dataset and trains our graphSAGE model for link prediction"
      ],
      "metadata": {
        "id": "E_jELAQBlOD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n"
      ],
      "metadata": {
        "id": "4bA28BNboFpD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC2uy_KilGnP"
      },
      "outputs": [],
      "source": [
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "import dgl\n",
        "import torch\n",
        "import numpy as np\n",
        "from dgl import AddReverse, Compose, ToSimple\n",
        "from dgl.nn import SAGEConv\n",
        "from tqdm import tqdm\n",
        "from numpy.random import default_rng\n",
        "from copy import deepcopy\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset download and graph setup"
      ],
      "metadata": {
        "id": "IF2o9PIqoOBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = DglNodePropPredDataset(name = \"ogbn-mag\", root = 'dataset/')\n",
        "graph,labels=dataset[0]\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
        "\n",
        "valid_idx['paper']=[x for x in valid_idx['paper'] if len(graph.predecessors(x,etype='writes'))>1] #since task is co-author prediction we choose papers with multiple authors\n",
        "k_hops=5 #sub graph size during train and eval\n",
        "train_graph= dgl.remove_nodes(graph, valid_idx['paper'],ntype='paper') #ensures no validation papers leak into train"
      ],
      "metadata": {
        "id": "0SS7EXp6oK-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train time subgrah sampler"
      ],
      "metadata": {
        "id": "6Fb54UiYogm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PaperNbrSampler(dgl.dataloading.Sampler):\n",
        "    def __init__(self, num_author_pair,khops):\n",
        "        super().__init__()\n",
        "        self.num_author_pair = num_author_pair #This is per graph\n",
        "        self.khops=khops\n",
        "\n",
        "\n",
        "    def sample_positive_author_pairs(self,graph,num_samples_per_graph):\n",
        "        num_graphs=graph.batch_size\n",
        "        positive_pairs=torch.zeros(num_samples_per_graph*num_graphs,2)\n",
        "\n",
        "        cfiller=0\n",
        "        anodes=graph.nodes(ntype='author')\n",
        "        while cfiller<(positive_pairs.shape[0]):\n",
        "            source=np.random.choice(anodes)\n",
        "            papers=graph.successors(source,etype='writes')\n",
        "            if len(papers)==0:\n",
        "                continue\n",
        "            paper=np.random.choice(papers)\n",
        "            writers=graph.predecessors(paper,etype='writes')\n",
        "            for writer in writers:\n",
        "                if writer!=source:\n",
        "                    positive_pairs[cfiller,0]=source\n",
        "                    positive_pairs[cfiller,1]=writer\n",
        "                    cfiller+=1\n",
        "                    break\n",
        "\n",
        "        return positive_pairs\n",
        "\n",
        "    def sample_negative_author_pairs(self,graph,sample_per_graph):\n",
        "        #Nodes of individual graphs are ordered by (graph,nodeID in graph) and then given one grand ordering which is what we use here.\n",
        "        anodes=graph.nodes(ntype='author')\n",
        "        cntauth=graph.batch_num_nodes('author')\n",
        "        num_graphs=cntauth.shape[0]\n",
        "        negative_pairs=torch.zeros(sample_per_graph*num_graphs,2)\n",
        "        idx2fillfrom=0\n",
        "        for i in range(num_graphs):\n",
        "            negative_pairs[sample_per_graph*i:sample_per_graph*(i+1),0]=torch.tensor(np.random.choice(anodes[idx2fillfrom:idx2fillfrom+cntauth[i]],sample_per_graph))\n",
        "            negative_pairs[sample_per_graph*i:sample_per_graph*(i+1),1]=torch.tensor(np.random.choice(anodes[idx2fillfrom:idx2fillfrom+cntauth[i]],sample_per_graph))\n",
        "            idx2fillfrom+=cntauth[i]\n",
        "        return negative_pairs\n",
        "\n",
        "    def sample(self,graph,indices):\n",
        "        #g is full graph. indices are the train paper nodes in curent mini batch\n",
        "        subgraphs=[]\n",
        "        for paper in indices:\n",
        "            sg=dgl.khop_subgraph(graph,{'paper':[paper]},self.khops)[0]\n",
        "            subgraphs.append(sg)\n",
        "        mini_batch=dgl.batch(subgraphs)\n",
        "        positive_pairs=None#self.sample_positive_author_pairs(mini_batch,self.num_author_pair)\n",
        "        negative_pairs=None#self.sample_negative_author_pairs(mini_batch,self.num_author_pair)\n",
        "        return mini_batch#,positive_pairs,negative_pairs"
      ],
      "metadata": {
        "id": "j3SSqkmBokXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation time paper sampler for loss computation"
      ],
      "metadata": {
        "id": "uUcRpNF2oqof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ValPaperNbrSampler(dgl.dataloading.Sampler):\n",
        "    def __init__(self,khops):\n",
        "        super().__init__()\n",
        "        self.khops=khops\n",
        "\n",
        "    def sample(self,graph,indices):\n",
        "        #g is full graph. It returns the index in indices and the author nodes with which edge exists in reality\n",
        "        #returns, OG subgraph, sub graph to do MSG passing on, correct edges\n",
        "        assert indices.shape[0]==1\n",
        "        sg,invlabel=dgl.khop_subgraph(graph,{'paper':[indices[0]]},self.khops)\n",
        "        paper_in_subg=invlabel['paper']\n",
        "        authors=sg.predecessors(paper_in_subg[0],etype='writes')\n",
        "        author_to_del=authors[:-1]\n",
        "        edge_ids_to_del=sg.edge_ids(author_to_del,paper_in_subg.repeat(len(author_to_del)),etype='writes')\n",
        "        sg_model_inp=dgl.remove_edges(sg, edge_ids_to_del,etype='writes')\n",
        "\n",
        "        sg_to_pred=deepcopy(sg)\n",
        "        for etype in sg.etypes:\n",
        "            sg_to_pred=dgl.remove_edges(sg_to_pred, sg.edges(form='eid',etype=etype),etype=etype)\n",
        "        all_authors=sg.nodes('author')\n",
        "        non_paper_authors=np.setdiff1d(all_authors,author_to_del)\n",
        "        neg_authors=np.random.choice(non_paper_authors,min(len(author_to_del)*2,non_paper_authors.shape[0]),replace=False)\n",
        "        authors_to_add=np.concatenate([neg_authors,author_to_del])\n",
        "        sg_to_pred.add_edges(authors_to_add,paper_in_subg.repeat(len(authors_to_add)),etype='writes')\n",
        "        return sg_model_inp,sg_to_pred,invlabel['paper'][0],author_to_del,authors[-1]\n",
        "\n",
        "\n",
        "class ValPaperNbrSamplerLoss(dgl.dataloading.Sampler):\n",
        "    def __init__(self,khops):\n",
        "        super().__init__()\n",
        "        self.khops=khops\n",
        "\n",
        "    def sample(self,graph,indices):\n",
        "        #g is full graph. It returns the index in indices and the author nodes with which edge exists in reality\n",
        "        #returns, OG subgraph, sub graph to do MSG passing on, correct edges\n",
        "        assert indices.shape[0]==1\n",
        "        sg,invlabel=dgl.khop_subgraph(graph,{'paper':[indices[0]]},self.khops)\n",
        "        paper_in_subg=invlabel['paper']\n",
        "        authors=sg.predecessors(paper_in_subg[0],etype='writes')\n",
        "        author_to_del=authors[:-1]\n",
        "        edge_ids_to_del=sg.edge_ids(author_to_del,paper_in_subg.repeat(len(author_to_del)),etype='writes')\n",
        "        sg_model_inp=dgl.remove_edges(sg, edge_ids_to_del,etype='writes')\n",
        "        # sg_model_inp=deepcopy(sg)\n",
        "        sg_pos=deepcopy(sg)\n",
        "        sg_neg=deepcopy(sg)\n",
        "        for etype in sg.etypes:\n",
        "            sg_pos=dgl.remove_edges(sg_pos, sg.edges(form='eid',etype=etype),etype=etype)\n",
        "            sg_neg=dgl.remove_edges(sg_neg, sg.edges(form='eid',etype=etype),etype=etype)\n",
        "        all_authors=sg.nodes('author')\n",
        "        neg_authors=np.random.choice(np.setdiff1d(all_authors,author_to_del),len(author_to_del))\n",
        "        sg_pos.add_edges(author_to_del,paper_in_subg.repeat(len(author_to_del)),etype='writes')\n",
        "        sg_neg.add_edges(neg_authors,paper_in_subg.repeat(len(neg_authors)),etype='writes')\n",
        "        return sg_model_inp,sg_pos,sg_neg"
      ],
      "metadata": {
        "id": "n3kxHHraovCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialise the dataloaders below"
      ],
      "metadata": {
        "id": "LPJHqCSIo_B5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coauth_train_loader = dgl.dataloading.DataLoader(\n",
        "        train_graph,\n",
        "        train_graph.nodes('paper'),\n",
        "        PaperNbrSampler(2,k_hops),\n",
        "        batch_size=1,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        device='cpu',\n",
        "    )\n",
        "\n",
        "coauth_val_loader_loss = dgl.dataloading.DataLoader(\n",
        "        graph,\n",
        "        valid_idx['paper'],\n",
        "        ValPaperNbrSamplerLoss(k_hops),\n",
        "        batch_size=1,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        device='cpu',\n",
        "    )"
      ],
      "metadata": {
        "id": "KqNzrSwfo9nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "yUrCAh2rpMHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_node_ids(node_type_list,num_node_func,node_type,node_ids):\n",
        "    #return the the corresponding node ID after homogenisation of a hetero graph\n",
        "    # num_node_func is a function that takes noode type as argument and returns number of nodes of that type\n",
        "    #node_type is the tpye of node on node_ids\n",
        "    #node_ids (array) whose homogenous node_id we need. All nodes in node_ids should be of the same type\n",
        "    i=0\n",
        "    ans=torch.clone(node_ids)\n",
        "    while node_type_list[i]!=node_type:\n",
        "        ans+=num_node_func(node_type_list[i])\n",
        "        i+=1\n",
        "    return ans\n",
        "\n",
        "def get_avg_num_shortest_paths(graph,source,targets):\n",
        "    g1=graph.to_networkx()\n",
        "    return np.mean([len([x for x in nx.all_shortest_paths(g1,source=source,target=i)]) for i in targets])\n",
        "\n",
        "def construct_all_inputs(graph,authors,papers,authors_neg,papers_neg):\n",
        "    #construct input from homogenous graph\n",
        "    #authors,papers are positive edges\n",
        "    #authors_neg,papers_neg are negative edges\n",
        "    author_paper_homo_edge_ids=graph.edge_ids(authors,papers)\n",
        "    author_paper_homo_edge_ids=torch.concatenate([author_paper_homo_edge_ids,author_paper_homo_edge_ids+graph.number_of_edges()//2])\n",
        "    graph_inp=dgl.remove_edges(graph, author_paper_homo_edge_ids)\n",
        "\n",
        "    graph_pos=dgl.remove_edges(graph,torch.arange(graph.number_of_edges())) #remove all edges\n",
        "    graph_pos=dgl.add_edges(graph_pos,authors,papers)\n",
        "    graph_pos=dgl.add_edges(graph_pos,papers,authors)\n",
        "\n",
        "    graph_neg=dgl.remove_edges(graph,torch.arange(graph.number_of_edges())) #remove all edges\n",
        "    graph_neg=dgl.add_edges(graph_neg,authors_neg,papers_neg)\n",
        "    graph_neg=dgl.add_edges(graph_neg,papers_neg,authors_neg)\n",
        "\n",
        "    return graph_inp,graph_pos,graph_neg\n",
        "\n",
        "def get_author_paper_pairs(graph,neg=False):\n",
        "    #graph is hetero,\n",
        "    #if neg is true return negative papers, else positive\n",
        "    src,dst=graph.edges(etype='writes')\n",
        "    num_writes=src.shape[0]\n",
        "    rng = default_rng()\n",
        "    eids = rng.choice(num_writes, size=num_writes//2, replace=False)\n",
        "    authors=src[eids]\n",
        "    if neg:\n",
        "        papers=dst[np.random.permutation(eids)]\n",
        "    else:\n",
        "        papers=dst[eids]\n",
        "    return authors,papers"
      ],
      "metadata": {
        "id": "i_cSjy_jpLVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation loop function"
      ],
      "metadata": {
        "id": "xiZsM0m5po6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model,val_loader):\n",
        "    #returns recall@10\n",
        "    model.eval()\n",
        "    recalls=[]\n",
        "    for i, (subg_inp,sg_to_pred,paper,authors,conn_author) in enumerate(val_loader):\n",
        "\n",
        "        if i>50:\n",
        "            break\n",
        "\n",
        "        ### homogenising each input node\n",
        "        paper_node_homo=find_node_ids(subg_inp.ntypes,subg_inp.num_nodes,'paper',paper)\n",
        "        author_node_homo=find_node_ids(subg_inp.ntypes,subg_inp.num_nodes,'author',authors)\n",
        "\n",
        "        conn_author_homo=find_node_ids(subg_inp.ntypes,subg_inp.num_nodes,'author',conn_author)\n",
        "\n",
        "        ###homogenising each input graph\n",
        "\n",
        "        subg_pred_homo=dgl.to_homogeneous(sg_to_pred)\n",
        "        subg_inp_homo=dgl.to_homogeneous(subg_inp)\n",
        "        sub_pred_undir=dgl.add_reverse_edges(subg_pred_homo)\n",
        "        sub_inp_undir=dgl.add_reverse_edges(subg_inp_homo)\n",
        "\n",
        "        ### one hot encoding of node type\n",
        "        node_feats=torch.zeros((subg_pred_homo.num_nodes(),len(subg_inp.ntypes))) #one hot encoding of node type\n",
        "        node_feats[torch.arange(subg_pred_homo.num_nodes()),subg_pred_homo.ndata['_TYPE']]=1\n",
        "        # edge_ids_to_predict=sub_pred_undir.edge_ids(author_node_homo,paper_node_homo.repeat(len(author_node_homo)))\n",
        "\n",
        "        ### running model\n",
        "        op=model(sub_inp_undir,None,node_feats,sub_pred_undir)\n",
        "        op=op[:(op.shape[0]//2)] #because symmetric edges\n",
        "        recall_k=min(int(np.ceil(authors.shape[0]*1.5)),10)\n",
        "\n",
        "        ###Recall@K#####\n",
        "        if len(op.shape)>1:\n",
        "            pred_edges=torch.topk(op[:,0],recall_k)[1]\n",
        "        else:\n",
        "            pred_edges=torch.topk(op,recall_k)[1]\n",
        "        pred_authors=sub_pred_undir.edges()[0][pred_edges]\n",
        "\n",
        "        recalls.append(np.intersect1d(pred_authors,author_node_homo).shape[0]/authors.shape[0])\n",
        "\n",
        "    return np.mean(recalls)\n",
        "\n",
        "def validate_w_loss(model,val_loader,loss_func):\n",
        "    #returns validation loss\n",
        "    model.eval()\n",
        "    losses=[]\n",
        "    for i, (subg_inp,sg_pos,sg_neg) in enumerate(val_loader):\n",
        "\n",
        "        if i>50:\n",
        "            break\n",
        "        # paper_node_homo=find_node_ids(subg_inp.ntypes,subg_inp.num_nodes,'paper',paper)\n",
        "        # author_node_homo=find_node_ids(subg_inp.ntypes,subg_inp.num_nodes,'author',authors)\n",
        "\n",
        "        ###homogenising each input graph\n",
        "\n",
        "        subg_pos_homo=dgl.to_homogeneous(sg_pos)\n",
        "        subg_neg_homo=dgl.to_homogeneous(sg_neg)\n",
        "        subg_inp_homo=dgl.to_homogeneous(subg_inp)\n",
        "        sub_pos_undir=dgl.add_reverse_edges(subg_pos_homo)\n",
        "        sub_neg_undir=dgl.add_reverse_edges(subg_neg_homo)\n",
        "        sub_inp_undir=dgl.add_reverse_edges(subg_inp_homo)\n",
        "\n",
        "         ### one hot encoding of node type\n",
        "        node_feats=torch.zeros((subg_inp_homo.num_nodes(),len(subg_inp.ntypes))) #one hot encoding of node type\n",
        "        node_feats[torch.arange(subg_inp_homo.num_nodes()),subg_inp_homo.ndata['_TYPE']]=1\n",
        "        # edge_ids_to_predict=sub_pred_undir.edge_ids(author_node_homo,paper_node_homo.repeat(len(author_node_homo)))\n",
        "\n",
        "        pos_score,neg_score=model(sub_inp_undir,sub_neg_undir,node_feats,sub_pos_undir)\n",
        "        loss = loss_func(pos_score, neg_score)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "\n",
        "    return np.mean(losses)"
      ],
      "metadata": {
        "id": "bU8hNGX7poGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model defintion"
      ],
      "metadata": {
        "id": "LO4ZR0MoqOnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPPredictor(torch.nn.Module):\n",
        "    def __init__(self, h_feats):\n",
        "        super().__init__()\n",
        "        self.W1 = torch.nn.Linear(h_feats * 2, h_feats)\n",
        "        self.W2 = torch.nn.Linear(h_feats, 1)\n",
        "\n",
        "    def apply_edges(self, edges):\n",
        "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
        "        return {'score': self.W2(torch.nn.functional.relu(self.W1(h))).squeeze(1)}\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            g.apply_edges(self.apply_edges)\n",
        "            return g.edata['score']\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, in_features, hidden_features):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sage1=SAGEConv(in_features, hidden_features, 'mean')\n",
        "        self.sage2=SAGEConv(hidden_features, hidden_features, 'mean')\n",
        "        self.sage3=SAGEConv(hidden_features, hidden_features, 'mean')\n",
        "\n",
        "        self.pred=MLPPredictor(hidden_features)\n",
        "        self.bn=torch.nn.BatchNorm1d(hidden_features, eps=1)\n",
        "    def forward(self, g, neg_g, x,pos_g=None):\n",
        "         #g should not have the edges we want to predict which are there in pos_g. We do message passing on g\n",
        "         #neg_g is the graph of negative edges\n",
        "         #x is node features\n",
        "         #pos_g is graph of positive edges\n",
        "        if pos_g is None:\n",
        "            pos_g=g\n",
        "        h = self.sage1(g, x)\n",
        "        h=self.sage2(g,h)\n",
        "        h=self.sage3(g,h)\n",
        "\n",
        "        if pos_g is not None and neg_g is not None:\n",
        "            return self.pred(pos_g, h), self.pred(neg_g, h)\n",
        "        elif neg_g is not None:\n",
        "            return self.pred(neg_g,h)\n",
        "        else:\n",
        "            return self.pred(pos_g,h)"
      ],
      "metadata": {
        "id": "go6bj-VIqQCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss functions"
      ],
      "metadata": {
        "id": "lxOOfEzbq5yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(pos_score, neg_score):\n",
        "    # Margin loss\n",
        "    n_edges = pos_score.shape[0]\n",
        "    return (1 - pos_score + neg_score.view(n_edges, -1)).clamp(min=0).mean()\n",
        "\n",
        "def compute_loss_CE(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score])\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "    return torch.nn.functional.binary_cross_entropy_with_logits(scores, labels)"
      ],
      "metadata": {
        "id": "YWeHbEQdq5SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model initialisation"
      ],
      "metadata": {
        "id": "AFlhI7PNrV69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(4, 4)\n",
        "opt = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "D9GGGEFerVXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "XYZoHNIOre0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (subg) in (pbar:= tqdm(enumerate(coauth_train_loader))):\n",
        "    model.train()\n",
        "    author_pred,paper_pred=get_author_paper_pairs(subg)\n",
        "    author_pred_neg,paper_pred_neg=get_author_paper_pairs(subg,neg=True)\n",
        "\n",
        "    ##homogenise graph\n",
        "    sub_homo=dgl.to_homogeneous(subg)\n",
        "    sub_homo_undir=dgl.add_reverse_edges(sub_homo)\n",
        "\n",
        "    ##create features\n",
        "    node_feats=torch.zeros((subg.num_nodes(),len(subg.ntypes))) #one hot encoding of node type\n",
        "    node_feats[torch.arange(subg.num_nodes()),sub_homo_undir.ndata['_TYPE']]=1\n",
        "\n",
        "    ##homoegenise nodes\n",
        "    author_pred_homo=find_node_ids(subg.ntypes,subg.num_nodes,'author',author_pred)\n",
        "    paper_pred_homo=find_node_ids(subg.ntypes,subg.num_nodes,'paper',paper_pred)\n",
        "    author_pred_neg_homo=find_node_ids(subg.ntypes,subg.num_nodes,'author',author_pred_neg)\n",
        "    paper_pred_neg_homo=find_node_ids(subg.ntypes,subg.num_nodes,'paper',paper_pred_neg)\n",
        "\n",
        "    ##create inputs\n",
        "    input_graph,positive_graph,negative_graph=construct_all_inputs(sub_homo_undir,author_pred_homo,paper_pred_homo,author_pred_neg_homo,paper_pred_neg_homo)\n",
        "\n",
        "    ##get score, compute loss and backpropagate loss\n",
        "    pos_score, neg_score = model(input_graph, negative_graph, node_feats,positive_graph)\n",
        "    loss = compute_loss(pos_score, neg_score)\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    pbar.set_description(f\"Loss:{loss.item():.4f},Edges:{input_graph.number_of_edges()}\")\n",
        "    if (i+1)%50==0:\n",
        "        print(validate_w_loss(model,coauth_val_loader_loss,compute_loss))"
      ],
      "metadata": {
        "id": "zY6RTq4UrTj5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}